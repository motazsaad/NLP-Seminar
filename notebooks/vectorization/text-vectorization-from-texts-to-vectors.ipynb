{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cd5150",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:03.744924Z",
     "iopub.status.busy": "2024-09-27T16:17:03.744438Z",
     "iopub.status.idle": "2024-09-27T16:17:04.655910Z",
     "shell.execute_reply": "2024-09-27T16:17:04.654891Z"
    },
    "papermill": {
     "duration": 0.925005,
     "end_time": "2024-09-27T16:17:04.658583",
     "exception": false,
     "start_time": "2024-09-27T16:17:03.733578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4b931",
   "metadata": {
    "papermill": {
     "duration": 0.008821,
     "end_time": "2024-09-27T16:17:04.676356",
     "exception": false,
     "start_time": "2024-09-27T16:17:04.667535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bag of Words BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d19cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:04.694479Z",
     "iopub.status.busy": "2024-09-27T16:17:04.693145Z",
     "iopub.status.idle": "2024-09-27T16:17:05.993059Z",
     "shell.execute_reply": "2024-09-27T16:17:05.991985Z"
    },
    "papermill": {
     "duration": 1.311688,
     "end_time": "2024-09-27T16:17:05.995711",
     "exception": false,
     "start_time": "2024-09-27T16:17:04.684023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['and' 'animals' 'are' 'at' 'barked' 'cat' 'cats' 'dog' 'dogs' 'great'\n",
      " 'love' 'loyal' 'mat' 'my' 'on' 'pets' 'sat' 'the']\n",
      "\n",
      "Bag of Words Representation:\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 2]\n",
      " [0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 2]\n",
      " [1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the sentences to create the BOW representation\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert the BOW representation to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(feature_names)\n",
    "print(\"\\nBag of Words Representation:\")\n",
    "print(bow_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f211ea",
   "metadata": {
    "papermill": {
     "duration": 0.007751,
     "end_time": "2024-09-27T16:17:06.011322",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.003571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb394d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.028079Z",
     "iopub.status.busy": "2024-09-27T16:17:06.027593Z",
     "iopub.status.idle": "2024-09-27T16:17:06.045558Z",
     "shell.execute_reply": "2024-09-27T16:17:06.044267Z"
    },
    "papermill": {
     "duration": 0.029078,
     "end_time": "2024-09-27T16:17:06.047830",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.018752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['and' 'animals' 'are' 'at' 'barked' 'cat' 'cats' 'dog' 'dogs' 'great'\n",
      " 'love' 'loyal' 'mat' 'my' 'on' 'pets' 'sat' 'the']\n",
      "\n",
      "TF-IDF Representation:\n",
      "[[0.         0.         0.         0.         0.         0.27222751\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.40648465 0.         0.40648465 0.         0.40648465 0.65589852]\n",
      " [0.         0.         0.         0.40648465 0.40648465 0.27222751\n",
      "  0.         0.40648465 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.65589852]\n",
      " [0.43429718 0.         0.35038823 0.         0.         0.\n",
      "  0.43429718 0.         0.35038823 0.43429718 0.         0.\n",
      "  0.         0.         0.         0.43429718 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.42799292\n",
      "  0.         0.         0.         0.         0.63907044 0.\n",
      "  0.         0.63907044 0.         0.         0.         0.        ]\n",
      " [0.         0.55032913 0.44400208 0.         0.         0.\n",
      "  0.         0.         0.44400208 0.         0.         0.55032913\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences to create the TF-IDF representation\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Convert the TF-IDF representation to an array\n",
    "tfidf_array = X.toarray()\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(feature_names)\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "print(tfidf_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e33bc8",
   "metadata": {
    "papermill": {
     "duration": 0.007401,
     "end_time": "2024-09-27T16:17:06.063029",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.055628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bigrams + TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cd1d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.081011Z",
     "iopub.status.busy": "2024-09-27T16:17:06.080237Z",
     "iopub.status.idle": "2024-09-27T16:17:06.091499Z",
     "shell.execute_reply": "2024-09-27T16:17:06.090458Z"
    },
    "papermill": {
     "duration": 0.023547,
     "end_time": "2024-09-27T16:17:06.094990",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.071443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Bigrams):\n",
      "['and dogs' 'are great' 'are loyal' 'at the' 'barked at' 'cat sat'\n",
      " 'cats and' 'dog barked' 'dogs are' 'great pets' 'love my' 'loyal animals'\n",
      " 'my cat' 'on the' 'sat on' 'the cat' 'the dog' 'the mat']\n",
      "\n",
      "TF-IDF Representation (Bigrams):\n",
      "[[0.         0.         0.         0.         0.         0.46369322\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.46369322 0.46369322 0.37410477 0.         0.46369322]\n",
      " [0.         0.         0.         0.46369322 0.46369322 0.\n",
      "  0.         0.46369322 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37410477 0.46369322 0.        ]\n",
      " [0.46369322 0.46369322 0.         0.         0.         0.\n",
      "  0.46369322 0.         0.37410477 0.46369322 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.70710678 0.\n",
      "  0.70710678 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.61418897 0.         0.         0.\n",
      "  0.         0.         0.49552379 0.         0.         0.61418897\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer instance for bigrams\n",
    "vectorizer_tfidf = TfidfVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the sentences to create the TF-IDF representation for bigrams\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(sentences)\n",
    "\n",
    "# Convert the TF-IDF representation to an array\n",
    "tfidf_array_bigrams = X_tfidf.toarray()\n",
    "\n",
    "# Get the feature names (bigrams)\n",
    "feature_names_tfidf_bigrams = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Bigrams):\")\n",
    "print(feature_names_tfidf_bigrams)\n",
    "print(\"\\nTF-IDF Representation (Bigrams):\")\n",
    "print(tfidf_array_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78052ea8",
   "metadata": {
    "papermill": {
     "duration": 0.007747,
     "end_time": "2024-09-27T16:17:06.110629",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.102882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BoW of Unigrams + Bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "935561fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.128666Z",
     "iopub.status.busy": "2024-09-27T16:17:06.127818Z",
     "iopub.status.idle": "2024-09-27T16:17:06.138141Z",
     "shell.execute_reply": "2024-09-27T16:17:06.137044Z"
    },
    "papermill": {
     "duration": 0.022437,
     "end_time": "2024-09-27T16:17:06.140945",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.118508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Unigrams and Bigrams):\n",
      "['and' 'and dogs' 'animals' 'are' 'are great' 'are loyal' 'at' 'at the'\n",
      " 'barked' 'barked at' 'cat' 'cat sat' 'cats' 'cats and' 'dog' 'dog barked'\n",
      " 'dogs' 'dogs are' 'great' 'great pets' 'love' 'love my' 'loyal'\n",
      " 'loyal animals' 'mat' 'my' 'my cat' 'on' 'on the' 'pets' 'sat' 'sat on'\n",
      " 'the' 'the cat' 'the dog' 'the mat']\n",
      "\n",
      "Bag of Words Representation (Unigrams and Bigrams):\n",
      "[[0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 2 1 0 1]\n",
      " [0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 0]\n",
      " [1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance for unigrams and bigrams\n",
    "vectorizer_bow = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the sentences to create the BOW representation for unigrams and bigrams\n",
    "X_bow = vectorizer_bow.fit_transform(sentences)\n",
    "\n",
    "# Convert the BOW representation to an array\n",
    "bow_array_bigrams = X_bow.toarray()\n",
    "\n",
    "# Get the feature names (unigrams and bigrams)\n",
    "feature_names_bigrams = vectorizer_bow.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Unigrams and Bigrams):\")\n",
    "print(feature_names_bigrams)\n",
    "print(\"\\nBag of Words Representation (Unigrams and Bigrams):\")\n",
    "print(bow_array_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70992bdc",
   "metadata": {
    "papermill": {
     "duration": 0.007498,
     "end_time": "2024-09-27T16:17:06.156553",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.149055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1gram + 2grams + 3grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2dca12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.174513Z",
     "iopub.status.busy": "2024-09-27T16:17:06.173624Z",
     "iopub.status.idle": "2024-09-27T16:17:06.184429Z",
     "shell.execute_reply": "2024-09-27T16:17:06.183218Z"
    },
    "papermill": {
     "duration": 0.022441,
     "end_time": "2024-09-27T16:17:06.186874",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.164433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Unigrams and Bigrams and Trigrams):\n",
      "['and' 'and dogs' 'and dogs are' 'animals' 'are' 'are great'\n",
      " 'are great pets' 'are loyal' 'are loyal animals' 'at' 'at the'\n",
      " 'at the cat' 'barked' 'barked at' 'barked at the' 'cat' 'cat sat'\n",
      " 'cat sat on' 'cats' 'cats and' 'cats and dogs' 'dog' 'dog barked'\n",
      " 'dog barked at' 'dogs' 'dogs are' 'dogs are great' 'dogs are loyal'\n",
      " 'great' 'great pets' 'love' 'love my' 'love my cat' 'loyal'\n",
      " 'loyal animals' 'mat' 'my' 'my cat' 'on' 'on the' 'on the mat' 'pets'\n",
      " 'sat' 'sat on' 'sat on the' 'the' 'the cat' 'the cat sat' 'the dog'\n",
      " 'the dog barked' 'the mat']\n",
      "\n",
      "Bag of Words Representation (Unigrams and Bigrams and Trigrams):\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 1 1 1 0 1 1 1 2 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 2 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance for unigrams and bigrams\n",
    "vectorizer_bow = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Fit and transform the sentences to create the BOW representation for unigrams and bigrams\n",
    "X_bow = vectorizer_bow.fit_transform(sentences)\n",
    "\n",
    "# Convert the BOW representation to an array\n",
    "bow_array_bigrams = X_bow.toarray()\n",
    "\n",
    "# Get the feature names (unigrams and bigrams)\n",
    "feature_names_bigrams = vectorizer_bow.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Unigrams and Bigrams and Trigrams):\")\n",
    "print(feature_names_bigrams)\n",
    "print(\"\\nBag of Words Representation (Unigrams and Bigrams and Trigrams):\")\n",
    "print(bow_array_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8d4c0",
   "metadata": {
    "papermill": {
     "duration": 0.008375,
     "end_time": "2024-09-27T16:17:06.203638",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.195263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF-IDF of Unigrams + Bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5910969d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.222031Z",
     "iopub.status.busy": "2024-09-27T16:17:06.221151Z",
     "iopub.status.idle": "2024-09-27T16:17:06.235213Z",
     "shell.execute_reply": "2024-09-27T16:17:06.233863Z"
    },
    "papermill": {
     "duration": 0.025881,
     "end_time": "2024-09-27T16:17:06.237707",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.211826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Unigrams and Bigrams):\n",
      "['and' 'and dogs' 'animals' 'are' 'are great' 'are loyal' 'at' 'at the'\n",
      " 'barked' 'barked at' 'cat' 'cat sat' 'cats' 'cats and' 'dog' 'dog barked'\n",
      " 'dogs' 'dogs are' 'great' 'great pets' 'love' 'love my' 'loyal'\n",
      " 'loyal animals' 'mat' 'my' 'my cat' 'on' 'on the' 'pets' 'sat' 'sat on'\n",
      " 'the' 'the cat' 'the dog' 'the mat']\n",
      "\n",
      "TF-IDF Representation (Unigrams and Bigrams):\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.20470723 0.30566473\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30566473 0.         0.         0.30566473 0.30566473 0.\n",
      "  0.30566473 0.30566473 0.49321676 0.24660838 0.         0.30566473]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.30566473 0.30566473 0.30566473 0.30566473 0.20470723 0.\n",
      "  0.         0.         0.30566473 0.30566473 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.49321676 0.24660838 0.30566473 0.        ]\n",
      " [0.31697754 0.31697754 0.         0.25573548 0.31697754 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31697754 0.31697754 0.         0.         0.25573548 0.25573548\n",
      "  0.31697754 0.31697754 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31697754\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3175268  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.47412465 0.47412465 0.         0.\n",
      "  0.         0.47412465 0.47412465 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.40986539 0.33067681 0.         0.40986539\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.33067681 0.33067681\n",
      "  0.         0.         0.         0.         0.40986539 0.40986539\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer instance for unigrams and bigrams\n",
    "vectorizer_tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the sentences to create the TF-IDF representation for unigrams and bigrams\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(sentences)\n",
    "\n",
    "# Convert the TF-IDF representation to an array\n",
    "tfidf_array_bigrams = X_tfidf.toarray()\n",
    "\n",
    "# Get the feature names (unigrams and bigrams)\n",
    "feature_names_tfidf_bigrams = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Display the results\n",
    "print(\"Feature Names (Unigrams and Bigrams):\")\n",
    "print(feature_names_tfidf_bigrams)\n",
    "print(\"\\nTF-IDF Representation (Unigrams and Bigrams):\")\n",
    "print(tfidf_array_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3a73f",
   "metadata": {
    "papermill": {
     "duration": 0.007875,
     "end_time": "2024-09-27T16:17:06.254055",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.246180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# build word2vec model using gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90426a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:06.273073Z",
     "iopub.status.busy": "2024-09-27T16:17:06.272010Z",
     "iopub.status.idle": "2024-09-27T16:17:16.715225Z",
     "shell.execute_reply": "2024-09-27T16:17:16.714161Z"
    },
    "papermill": {
     "duration": 10.45549,
     "end_time": "2024-09-27T16:17:16.717867",
     "exception": false,
     "start_time": "2024-09-27T16:17:06.262377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      "[-0.00714226  0.00124265 -0.00717659 -0.00224207  0.00371794  0.00583205\n",
      "  0.00119609  0.00210415 -0.00411239  0.00722918 -0.00630918  0.00464433\n",
      " -0.00822246  0.00203907 -0.00497988 -0.00425001 -0.00311196  0.00565796\n",
      "  0.0058002  -0.00497406  0.00077228 -0.00849441  0.00780826  0.00925741\n",
      " -0.00274478  0.00079992  0.00074601  0.00548158 -0.0086069   0.00058133\n",
      "  0.00686806  0.00222932  0.00112576 -0.00931956  0.00848104 -0.00626174\n",
      " -0.00298954  0.00349149 -0.00077597  0.00141491  0.00178519 -0.00683123\n",
      " -0.0097282   0.00904432  0.00620004 -0.0069106   0.00340107  0.00020258\n",
      "  0.00475476 -0.0071242   0.00402555  0.00434663  0.00996184 -0.00447107\n",
      " -0.00139039 -0.00731593 -0.00969583 -0.00908193 -0.00101934 -0.00650131\n",
      "  0.00485089 -0.00616605  0.00251752  0.0007384  -0.00338965 -0.00098158\n",
      "  0.009979    0.0091501  -0.00446099  0.00908617 -0.00564525  0.00593122\n",
      " -0.00309551  0.00343033  0.00302022  0.00690447 -0.00237422  0.0087751\n",
      "  0.00758993 -0.00955069 -0.00801209 -0.00763914  0.00292255 -0.00279338\n",
      " -0.00693271 -0.00812706  0.00830752  0.00198776 -0.00932582 -0.00478991\n",
      "  0.0031357  -0.00471245  0.00527927 -0.00423703  0.00264002 -0.00804401\n",
      "  0.00621281  0.00482285  0.00078986  0.0030162 ]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "[('and', 0.2528415024280548), ('are', 0.17018046975135803), ('loyal', 0.15016357600688934), ('sat', 0.13889706134796143), ('i', 0.10852697491645813)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Preprocess the sentences: tokenize and convert to lowercase\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Build the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model (if needed)\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "word_vector = model.wv['cat']\n",
    "print(\"Vector for 'cat':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example usage: Find similar words\n",
    "similar_words = model.wv.most_similar('cat', topn=5)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc907a",
   "metadata": {
    "papermill": {
     "duration": 0.00838,
     "end_time": "2024-09-27T16:17:16.734654",
     "exception": false,
     "start_time": "2024-09-27T16:17:16.726274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# vector size 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1383bf35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:16.753194Z",
     "iopub.status.busy": "2024-09-27T16:17:16.752744Z",
     "iopub.status.idle": "2024-09-27T16:17:16.775022Z",
     "shell.execute_reply": "2024-09-27T16:17:16.773762Z"
    },
    "papermill": {
     "duration": 0.034323,
     "end_time": "2024-09-27T16:17:16.777216",
     "exception": false,
     "start_time": "2024-09-27T16:17:16.742893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat':\n",
      "[-0.09601653  0.05004316 -0.08760495 -0.04390959 -0.00035496 -0.0029315\n",
      " -0.07658748  0.09618056  0.04982246  0.09236773]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "[('on', 0.6144760847091675), ('i', 0.34242185950279236), ('sat', 0.24946902692317963), ('loyal', 0.14185985922813416), ('and', 0.11192134022712708)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Preprocess the sentences: tokenize and convert to lowercase\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "\n",
    "# Build the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "# Load the model (if needed)\n",
    "# model = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "word_vector = model.wv['cat']\n",
    "print(\"Vector for 'cat':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example usage: Find similar words\n",
    "similar_words = model.wv.most_similar('cat', topn=5)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b33a4",
   "metadata": {
    "papermill": {
     "duration": 0.00828,
     "end_time": "2024-09-27T16:17:16.794215",
     "exception": false,
     "start_time": "2024-09-27T16:17:16.785935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# build word2vec model using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8bef8aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:16.813666Z",
     "iopub.status.busy": "2024-09-27T16:17:16.813155Z",
     "iopub.status.idle": "2024-09-27T16:17:34.083665Z",
     "shell.execute_reply": "2024-09-27T16:17:34.082050Z"
    },
    "papermill": {
     "duration": 17.283452,
     "end_time": "2024-09-27T16:17:34.086223",
     "exception": false,
     "start_time": "2024-09-27T16:17:16.802771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.9969\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9868 \n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9731 \n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9608 \n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9486 \n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9446 \n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9293 \n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9151 \n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9070 \n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8913 \n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8774 \n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8619 \n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8537 \n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8368 \n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8158 \n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7947 \n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.8059 \n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7852 \n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7602 \n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7369 \n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7333 \n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7102 \n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7148 \n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6791 \n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6545 \n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6436 \n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6423 \n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5796 \n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5677 \n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5512 \n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5430 \n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5197 \n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4760 \n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5045 \n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4896 \n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4795 \n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4503 \n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4336 \n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4087 \n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3908 \n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3982 \n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3864 \n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3809 \n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3474 \n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3183 \n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3303 \n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3278 \n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3227 \n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2635 \n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3084 \n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2770 \n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2764 \n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2466 \n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2545 \n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2454 \n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2047 \n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2268 \n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1648 \n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2141 \n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1907 \n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1969 \n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2343 \n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1820 \n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1928 \n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1701 \n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1832 \n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2006 \n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1285 \n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1674 \n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1228 \n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1343 \n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1192 \n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1334 \n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1332 \n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1240 \n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0954 \n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1280 \n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1337 \n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1333 \n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1502 \n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1359 \n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1384 \n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1141 \n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1454 \n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1450 \n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1042 \n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1136 \n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0470 \n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0635 \n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1125 \n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0591 \n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0370 \n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0592 \n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0375 \n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0796 \n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0282 \n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0493 \n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0575 \n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0993 \n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0459 \n",
      "Vector for 'cat':\n",
      "[ 0.16536945 -0.01418473 -0.03339871 -0.2266488  -0.08812097  0.06712344\n",
      "  0.05885463  0.08691808 -0.05527977 -0.04909601 -0.05651233 -0.20688169\n",
      "  0.08312123 -0.1088989  -0.04701545 -0.05722913 -0.02097734 -0.00638386\n",
      " -0.18549976  0.07704486 -0.10736712  0.21597725  0.03978396  0.27581677\n",
      "  0.04561207 -0.06608655 -0.21092278 -0.13906959  0.1520697   0.00736481\n",
      "  0.05361263 -0.08858503 -0.05191712 -0.133318    0.00585841 -0.00478643\n",
      " -0.06912666 -0.12971598 -0.08273397 -0.30455628 -0.23881944  0.01973803\n",
      "  0.07979966  0.00377466  0.0150644  -0.14716268  0.06849134 -0.00668607\n",
      "  0.07497776 -0.02107215  0.03226044 -0.07024179  0.06857841 -0.3587315\n",
      " -0.08129851 -0.01012057 -0.16666172  0.15524158 -0.19956227  0.19627942\n",
      " -0.01590843  0.08408737 -0.13863602  0.01674855 -0.01393085  0.22476518\n",
      " -0.10295932  0.03885638 -0.02386491 -0.07686552 -0.00241795  0.05042563\n",
      "  0.28194913 -0.03115795  0.23390232  0.03244043 -0.24662927  0.04820409\n",
      " -0.0745848  -0.12266675 -0.09415349 -0.05654039 -0.13054745  0.09604874\n",
      " -0.2804548   0.14212368  0.09029426  0.24756294  0.04126982  0.10848213\n",
      " -0.15367553  0.02607834 -0.01076321 -0.31551233 -0.03803961 -0.0587398\n",
      " -0.09232287  0.1470156  -0.01608886 -0.22011285]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "[('barked', 1.3478849), ('i', 1.1059076), ('loyal', 0.81916094), ('mat', 0.47806126), ('on', 0.44146705)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\",\n",
    "    \"Cats and dogs are great pets.\",\n",
    "    \"I love my cat.\",\n",
    "    \"Dogs are loyal animals.\"\n",
    "]\n",
    "\n",
    "# Preprocess the sentences: tokenize and convert to lowercase\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "total_words = len(tokenizer.word_index) + 1  # +1 for padding\n",
    "\n",
    "# Create skip-grams\n",
    "input_words, output_words = [], []\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    pairs, _ = skipgrams(tokenized_sentence, vocabulary_size=total_words, window_size=2, shuffle=True)\n",
    "    input_words.extend([pair[0] for pair in pairs])\n",
    "    output_words.extend([pair[1] for pair in pairs])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "input_words = np.array(input_words)\n",
    "output_words = np.array(output_words)\n",
    "\n",
    "# Build the Word2Vec model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=1, name='embedding_layer'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_words, output_words, epochs=100, verbose=1)\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "word_index = tokenizer.word_index['cat']\n",
    "word_vector = model.layers[0].get_weights()[0][word_index]\n",
    "print(\"Vector for 'cat':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example usage: Find similar words\n",
    "def find_similar_words(word, top_n=5):\n",
    "    word_vector = model.layers[0].get_weights()[0][tokenizer.word_index[word]]\n",
    "    similarities = {}\n",
    "    for w, idx in tokenizer.word_index.items():\n",
    "        if w != word:\n",
    "            # use cosine instead of dot product \n",
    "            sim = np.dot(word_vector, model.layers[0].get_weights()[0][idx])\n",
    "            similarities[w] = sim\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return similar_words\n",
    "\n",
    "similar_words = find_similar_words('cat')\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd5b27",
   "metadata": {
    "papermill": {
     "duration": 0.036706,
     "end_time": "2024-09-27T16:17:34.316512",
     "exception": false,
     "start_time": "2024-09-27T16:17:34.279806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# use Gensim pre-trained embedding models \n",
    "\n",
    "* https://github.com/piskvorky/gensim-data\n",
    "* https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93934f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:17:34.391172Z",
     "iopub.status.busy": "2024-09-27T16:17:34.390756Z",
     "iopub.status.idle": "2024-09-27T16:22:38.208576Z",
     "shell.execute_reply": "2024-09-27T16:22:38.207396Z"
    },
    "papermill": {
     "duration": 316.155244,
     "end_time": "2024-09-27T16:22:50.508170",
     "exception": false,
     "start_time": "2024-09-27T16:17:34.352926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Vector for 'cat':\n",
      "[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
      " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
      " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
      " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
      " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
      " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
      "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
      " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
      "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
      "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
      "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
      " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
      "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
      " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
      " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
      "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
      "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
      "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
      " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
      " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
      " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
      " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
      "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
      "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
      "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
      "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
      "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
      "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
      " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
      " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
      "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
      "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
      " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
      " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
      " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
      "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
      " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
      "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
      " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
      "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
      "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
      " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
      " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
      "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
      " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
      " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
      " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
      "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
      "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "[('cats', 0.8099379539489746), ('dog', 0.760945737361908), ('kitten', 0.7464985251426697), ('feline', 0.7326234579086304), ('beagle', 0.7150582671165466)]\n",
      "\n",
      "Similarity between 'cat' and 'dog':\n",
      "0.76094574\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download and load the pre-trained Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "word_vector = model['cat']\n",
    "print(\"Vector for 'cat':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example usage: Find similar words\n",
    "similar_words = model.most_similar('cat', topn=5)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "print(similar_words)\n",
    "\n",
    "# Example usage: Calculate similarity between two words\n",
    "similarity = model.similarity('cat', 'dog')\n",
    "print(\"\\nSimilarity between 'cat' and 'dog':\")\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf7657",
   "metadata": {
    "papermill": {
     "duration": 14.046584,
     "end_time": "2024-09-27T16:23:18.542849",
     "exception": false,
     "start_time": "2024-09-27T16:23:04.496265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GloVe pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79944118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T16:23:47.471113Z",
     "iopub.status.busy": "2024-09-27T16:23:47.470602Z",
     "iopub.status.idle": "2024-09-27T16:24:57.087317Z",
     "shell.execute_reply": "2024-09-27T16:24:57.077252Z"
    },
    "papermill": {
     "duration": 100.43218,
     "end_time": "2024-09-27T16:25:12.897585",
     "exception": false,
     "start_time": "2024-09-27T16:23:32.465405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "Vector for 'cat':\n",
      "[-0.96419  -0.60978   0.67449   0.35113   0.41317  -0.21241   1.3796\n",
      "  0.12854   0.31567   0.66325   0.3391   -0.18934  -3.325    -1.1491\n",
      " -0.4129    0.2195    0.8706   -0.50616  -0.12781  -0.066965  0.065761\n",
      "  0.43927   0.1758   -0.56058   0.13529 ]\n",
      "\n",
      "Most similar words to 'cat':\n",
      "[('dog', 0.9590820074081421), ('monkey', 0.920357882976532), ('bear', 0.9143136739730835), ('pet', 0.9108031392097473), ('girl', 0.8880629539489746)]\n",
      "\n",
      "Similarity between 'cat' and 'dog':\n",
      "0.95908207\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download and load the pre-trained gLoVe model\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# Example usage: Get the vector for a word\n",
    "word_vector = model['cat']\n",
    "print(\"Vector for 'cat':\")\n",
    "print(word_vector)\n",
    "\n",
    "# Example usage: Find similar words\n",
    "similar_words = model.most_similar('cat', topn=5)\n",
    "print(\"\\nMost similar words to 'cat':\")\n",
    "print(similar_words)\n",
    "\n",
    "# Example usage: Calculate similarity between two words\n",
    "similarity = model.similarity('cat', 'dog')\n",
    "print(\"\\nSimilarity between 'cat' and 'dog':\")\n",
    "print(similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 510.329248,
   "end_time": "2024-09-27T16:25:31.222374",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-27T16:17:00.893126",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
